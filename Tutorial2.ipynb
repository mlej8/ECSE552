{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T2-DoItYourself.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlej8/ECSE552/blob/main/Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2f6qYmvu_cV"
      },
      "source": [
        "# Multi-Layer Perceptron Using PyTorch\n",
        "\n",
        "Goals:\n",
        " - Learn PyTorch's Data Utilities\n",
        " - Learn PyTorch's built-in neural network functions instead of implementing them from scratch\n",
        " - Learn how to use the GPU for computation\n",
        " - Learn how to access the model's parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8lrs74vN7C9"
      },
      "source": [
        "**NOTE:** to use the GPU in Colab, click Edit > Notebook Settings > GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeoe9LUmvLrS"
      },
      "source": [
        "## Part 0: Preparing Datasets in PyTorch\n",
        "\n",
        "A lot of tutorials out there start by teaching you how to classify MNIST digits dataset using neural networks. However, there's a lot of confusion when it comes to using these in your own data. This is because most of the time, they jump to the part where they just load the data which are already split into train-test or train-validation-test sets. \n",
        "\n",
        "So here, let's create a simple dataset using numpy first, then we will transform them into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNXOC31ovEsD"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl1AsHE7va8T"
      },
      "source": [
        "We will create two classes, where each class came from a normal distribution centered at (-1, 1) and (1, -1), with stdev of 0.7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWaQswm7vc1A"
      },
      "source": [
        "x1 = np.random.normal(loc=(-1,1), scale=0.7, size=(100,2))\n",
        "x2 = np.random.normal(loc=(1,-1), scale=0.7, size=(100,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wMvSbMLvh5B"
      },
      "source": [
        "Visualize the dataset just so we know we have created what we had in mind."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WqxUnR0vg37"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x1[:,0], x1[:,1], label='class0')\n",
        "plt.scatter(x2[:,0], x2[:,1], label='class1', marker='s')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vnNmfMuvu9F"
      },
      "source": [
        "x = np.concatenate([x1, x2], axis=0)\n",
        "y = np.ones(200)\n",
        "y[:100] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ap8bLJEv5F6"
      },
      "source": [
        "Now, we are ready to prepare to create training splits using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWm-xiPVv0U_"
      },
      "source": [
        "data = torch.utils.data.TensorDataset(\n",
        "            torch.Tensor(x),\n",
        "            torch.Tensor(y))\n",
        "data[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO948Xdyv7w_"
      },
      "source": [
        "train, val, test = torch.utils.data.random_split(data, lengths=[100, 50, 50])\n",
        "print(len(train), len(val), len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baPFlbRHwE-p"
      },
      "source": [
        "Now you could easily iterate through these dataset in batches but there is actually an easier way of iterating through your data without worrying about indexing, shuffling, and epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwyK0qABv_D3"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val, batch_size=25, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOLdi4axwVah"
      },
      "source": [
        "There are 2 commons ways of iterating through dataloader: \n",
        " - epoch-based (most common)\n",
        " - manual next() trigger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYwXbAUDwCuv"
      },
      "source": [
        "num_epoch = 3\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        print(i, x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhPfH_fGwd_e"
      },
      "source": [
        "num_steps = 30\n",
        "trigger_steps = len(train_loader) # number of batches in one epoch\n",
        "print(trigger_steps)\n",
        "for step in range(num_steps):\n",
        "    if step % trigger_steps == 0:\n",
        "        print('trigger')\n",
        "        tl = iter(train_loader)\n",
        "        \n",
        "    x, y = next(tl)\n",
        "    print(step, x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSV1oeuQw7zh"
      },
      "source": [
        "## Part 1: Learning XOR\n",
        "\n",
        "The XOR problem is a well-known example in which a perceptron is not able to learn the correct function.\n",
        "However, a simple stacking of perceptrons could easily solve the problem.\n",
        "In this tutorial we will show how to create a 2-layer perceptron and learn the XOR function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYXiF5nFwsbt"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "x_data = [[0,0], [0,1], [1,0], [1,1]]\n",
        "y_data = [[0], [1], [1], [0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcbQPgWNxFeI"
      },
      "source": [
        "First, we prepare the dataset. We are trying to learn the XOR function which only has 4 possible datapoints. In this example we train and test using the same datapoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qplKDWI_xEnC"
      },
      "source": [
        "x_data = torch.Tensor(x_data)\n",
        "y_data = torch.Tensor(y_data)\n",
        "data = torch.utils.data.TensorDataset(x_data, y_data)\n",
        "data = torch.utils.data.DataLoader(data, batch_size=1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I6IjqPsxYJJ"
      },
      "source": [
        "Next, we create a model. Instead of implementing the intialization of the parameters and the matrix multiplications, we can just create a module. There are mulitple ways of defining your model, but this way will give you a lot of flexibility in the forward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_3XIcwzxbXk"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_in, n_hidden)\n",
        "        self.layer2 = nn.Linear(n_hidden, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layer2(torch.relu(self.layer1(x)))\n",
        "\n",
        "model = MLP(n_in=2, n_hidden=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow9mmub8xvi1"
      },
      "source": [
        "Then, we create an optimizer. For this example, we will use the Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv29KGmcx2Hm"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) # the lr is typically smaller than this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3jf_m_Mx9H-"
      },
      "source": [
        "Now, we are ready to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukiC8Hsox_FW"
      },
      "source": [
        "n_epoch = 50\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    # training\n",
        "    model.train()\n",
        "    for x,y in data:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(x)\n",
        "        loss = F.mse_loss(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # validation/test\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_hat = model(x_data)\n",
        "        loss = F.mse_loss(y_hat, y_data)\n",
        "        acc = ((y_hat > 0.5) == y_data).float().mean()\n",
        "\n",
        "    print('%d: XOR(0,0)=%.4f XOR(0,1)=%.4f XOR(1,0)=%.4f XOR(1,1)=%.4f cost=%.4f, accuracy=%.2f'\\\n",
        "        %(epoch+1, y_hat[0], y_hat[1], y_hat[2], y_hat[3], loss.item(), acc.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOINi8utyN9K"
      },
      "source": [
        "## Part 2: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ0PSDOPy9eK"
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "len(mnist_trainset), len(mnist_testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0FLjK9SzTwA"
      },
      "source": [
        "Let's split our training data to training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBgxBe-izPgS"
      },
      "source": [
        "mnist_trainset, mnist_valset = torch.utils.data.random_split(mnist_trainset, lengths=[50000, 10000])\n",
        "\n",
        "train_data = torch.utils.data.DataLoader(mnist_trainset, batch_size=256, shuffle=True)\n",
        "val_data = torch.utils.data.DataLoader(mnist_valset, batch_size=256, shuffle=False)\n",
        "test_data = torch.utils.data.DataLoader(mnist_testset, batch_size=256, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr7qqzyz0B4e"
      },
      "source": [
        "Let's take a look at our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eonpgCd0BRi"
      },
      "source": [
        "samples = iter(test_data)\n",
        "(x, y) = next(samples)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNDUZCz_0nhU"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z93rbgP0pGN"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(x[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"y = {}\".format(y[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo_RRdIjMpfX"
      },
      "source": [
        "# DIY \\#1\n",
        "Create a ``nn.Module`` class with the following architecture:\n",
        "input &rightarrow; hidden &rightarrow; output\n",
        "\n",
        "The model will have 10 outputs and the input and and hidden layers are parameters to be set at intialization (i.e. we the following will be called: ``model = MnistMLP(n_in=28*28, n_hidden=500)``)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Wt60ylGFPS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yJRkX_WGRfi"
      },
      "source": [
        "In this example we also try to use the GPU (if it is available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fjEEbPAGKD-"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaWZlRhEGNBa"
      },
      "source": [
        "model = MnistMLP(n_in=28*28, n_hidden=500).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHUu-U0qGZWG"
      },
      "source": [
        "Create the loss function and optimizer. In this case, since we did not add a ``LogSoftmax()`` or a ``Softmax()`` layer in our model, we will use the ``CrossEntropyLoss()``. This combines the ``LogSoftmax`` function and ``NLLLoss``, which we use in classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsKsfrzIGZ53"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "val_loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uiDK2aYOYE7"
      },
      "source": [
        "## DIY \\# 2\n",
        "Create the training loop. The validation loop has already been filled. It would be somewhat similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEYYtmp-Gc-b"
      },
      "source": [
        "n_epoch = 10\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    \n",
        "    # training loop here\n",
        "        \n",
        "    model.eval() # signal evaluation phase\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "        for x, y in val_data:\n",
        "            x, y = x.to(device), y.to(device) # move the data to cuda\n",
        "            y_hat = model(x.view(-1,28*28))\n",
        "            val_loss += val_loss_func(y_hat, y)\n",
        "            \n",
        "            y_hat = torch.argmax(y_hat, axis=1)\n",
        "            val_acc += (y_hat == y).float().sum()\n",
        "        val_loss /= 10000\n",
        "        val_acc /= 10000\n",
        "    \n",
        "    print(\"%d\\tbatch-loss: %.4f\\tbatch-acc: %.4f\\tval-loss: %.4f\\tval-acc: %.4f\"%(\n",
        "        epoch, batch_loss, batch_acc, val_loss, val_acc))\n",
        "            \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFlQj164HOws"
      },
      "source": [
        "## Accessing Parameters and Plotting Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6710on6HYBV"
      },
      "source": [
        "for params in model.parameters():\n",
        "    print(params.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpulsIV7H2Zx"
      },
      "source": [
        "class MnistMLPEmbed(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden):\n",
        "        super(MnistMLPEmbed, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_in, n_hidden)\n",
        "        # self.layer2 = nn.Linear(n_hidden, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return torch.relu(self.layer1(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27pZfwllJe-s"
      },
      "source": [
        "embed_model = MnistMLPEmbed(28*28, 500)\n",
        "embed_model.load_state_dict(model.state_dict(), strict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_syWF1tJ3JU"
      },
      "source": [
        "samples = iter(test_data)\n",
        "(x, y) = next(samples)\n",
        "y = y.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnXliQVRKKD_"
      },
      "source": [
        "embedding = embed_model(x.view(-1,28*28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARIEdw8DKRxA"
      },
      "source": [
        "embedding.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWxvO518KViw"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "x = PCA(n_components=2).fit_transform(embedding.detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4n5ZggxLXDh"
      },
      "source": [
        "for i in range(10):\n",
        "  idx = np.where(y == i) \n",
        "  plt.scatter(x[idx,0], x[idx,1], label=i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1KU6hktLTnk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}